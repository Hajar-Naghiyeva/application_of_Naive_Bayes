{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aze\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aze\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aze\\Desktop\\application_of_Naive_Bayes...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# APPLIES TO BOTH OF THE IMPLEMENTATIONS\n",
    "\n",
    "# Importing necessary libraries \n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import math \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt') \n",
    "# Specifying the path as the second argument \n",
    "nltk.download('stopwords', download_dir=r'C:\\Users\\aze\\Desktop\\application_of_Naive_Bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLIES TO BOTH OF THE IMPLEMENTATIONS\n",
    "\n",
    "# Function to load movie reviews from a directory\n",
    "def load_reviews(directory):\n",
    "    reviews = []  # List to store review texts\n",
    "    labels = []  # List to store review labels (pos/neg)\n",
    "\n",
    "    # Looping over the directories 'pos' and 'neg'\n",
    "    for label in [\"pos\", \"neg\"]:\n",
    "        directory_name = os.path.join(directory, label)  # Path to the sub-directory\n",
    "        # Iterating over the files in the directory\n",
    "        for filename in os.listdir(directory_name):\n",
    "            # Checking if the file is a text file\n",
    "            if filename.endswith(\".txt\"):\n",
    "                # Opening the file for reading with proper encoding\n",
    "                with open(os.path.join(directory_name, filename), 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()  # Reading the content of the file\n",
    "                    reviews.append(text)  # Appending the text to the reviews list\n",
    "                    labels.append(label)  # Appending the label to the labels list\n",
    "\n",
    "    # Returning the list of reviews and their corresponding labels\n",
    "    return reviews, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLIES TO BOTH OF THE IMPLEMENTATIONS\n",
    "\n",
    "reviews, labels = load_reviews(r'C:\\Users\\aze\\Desktop\\application_of_Naive_Bayes\\txt_sentoken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review label\n",
      "0  films adapted from comic books have had plenty...   pos\n",
      "1  every now and then a movie comes along from a ...   pos\n",
      "2  you've got mail works alot better than it dese...   pos\n",
      "3   \" jaws \" is a rare film that grabs your atten...   pos\n",
      "4  moviemaking is a lot like being the general ma...   pos\n",
      "review    0\n",
      "label     0\n",
      "dtype: int64\n",
      "count     2000.000000\n",
      "mean      3893.002000\n",
      "std       1712.425852\n",
      "min         91.000000\n",
      "25%       2737.750000\n",
      "50%       3622.500000\n",
      "75%       4720.250000\n",
      "max      14957.000000\n",
      "Name: review, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# APPLIES TO BOTH OF THE IMPLEMENTATIONS\n",
    "\n",
    "# Converting the lists of reviews and labels into a DataFrame for easier manipulation\n",
    "data = pd.DataFrame({\n",
    "    'review': reviews,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "\n",
    "# Assuming 'data' is your DataFrame - dataset inspection \n",
    "print(data.head())  # Displaying the first few reviews\n",
    "print(data.isnull().sum())  # Checking for missing values\n",
    "print(data['review'].apply(len).describe())  # Analyzing review lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLIES TO BOTH OF THE IMPLEMENTATIONS\n",
    "\n",
    "# Setting of English stopwords\n",
    "english_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLIES TO BOTH OF THE IMPLEMENTATIONS\n",
    "\n",
    "# Function to preprocess text data with a tokenizer\n",
    "def preprocess_text(text):\n",
    "    # Removing non-alphanumeric characters\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    # Tokenizing text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Removing stopwords\n",
    "    tokens = [token for token in tokens if token not in english_stopwords]\n",
    "    # Rejoining all tokens back into a single string\n",
    "    text = ' '.join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLIES TO BOTH OF THE IMPLEMENTATIONS\n",
    "\n",
    "# Preprocessing each review in the dataset\n",
    "preprocessed_reviews = [preprocess_text(review) for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLIES TO MANUAL IMPLEMENTATION ONLY \n",
    "\n",
    "# Separating the pre-processed reviews in positive and negative lists based on the corresponding labels \n",
    "positive_reviews = []\n",
    "negative_reviews = []\n",
    "\n",
    "for review, label in zip(preprocessed_reviews, labels):\n",
    "    if label == 'pos':  \n",
    "        positive_reviews.append(review)\n",
    "    elif label == 'neg': \n",
    "        negative_reviews.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8025\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.82      0.78      0.80       203\n",
      "         pos       0.78      0.83      0.80       197\n",
      "\n",
      "    accuracy                           0.80       400\n",
      "   macro avg       0.80      0.80      0.80       400\n",
      "weighted avg       0.80      0.80      0.80       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# APPLIES TO LIBRARY IMPLEMENTATION ONLY\n",
    "\n",
    "# 'labels' is a list of corresponding labels (e.g., 'pos' or 'neg').\n",
    "\n",
    "# splitting the dataset into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(preprocessed_reviews, labels, test_size=0.2)\n",
    "\n",
    "\n",
    "# Initialize the CountVectorizer with the custom tokenizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# # converting the text documents to a matrix of token counts\n",
    "# vectorizer = CountVectorizer()\n",
    "X_train_counters = vectorizer.fit_transform(X_train)\n",
    "X_test_counters = vectorizer.transform(X_test)\n",
    "\n",
    "# training a Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_counters, Y_train)\n",
    "\n",
    "# making predictions on the test set\n",
    "Y_predicted = clf.predict(X_test_counters)\n",
    "\n",
    "# evaluating the classifier\n",
    "accuracy = accuracy_score(Y_test, Y_predicted)\n",
    "report = classification_report(Y_test, Y_predicted)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.66      0.82      0.73     67129\n",
      "         pos       0.80      0.63      0.71     76907\n",
      "\n",
      "    accuracy                           0.72    144036\n",
      "   macro avg       0.73      0.73      0.72    144036\n",
      "weighted avg       0.73      0.72      0.72    144036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# APPLIES TO MANUAL IMPLEMENTATION ONLY\n",
    "\n",
    "# Creating dictionaries to count the frequency of each token among the reviews.\n",
    "# Each word is counted only once per review to avoid bias towards longer reviews.\n",
    "positive_dictionary = {}\n",
    "negative_dictionary = {}\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    " \n",
    "# Specifying the number of training data to take \n",
    "# Here we calculate 80% of the data for training\n",
    "positive_count_train = round(0.8 * len(positive_reviews))\n",
    "negative_count_train = round(0.8 * len(negative_reviews))\n",
    "\n",
    "# Counter for positive and negative tokens in the training data\n",
    "positive_token_count = 0\n",
    "negative_token_count = 0\n",
    "\n",
    "# Total count of reviews for positive and negative training data\n",
    "total_count = positive_count_train + negative_count_train\n",
    "\n",
    "# Splitting the the positive reviews into training and testing sets (with a random seed)\n",
    "positive_train, positive_test = train_test_split(\n",
    "    positive_reviews, test_size=0.2\n",
    ")\n",
    "\n",
    "\n",
    "for review in positive_train:  \n",
    "  tokens = review.split()\n",
    "  # Counting the total number of positive tokens\n",
    "  positive_token_count += len(tokens)\n",
    "  # 'used' is a list to keep track of tokens already seen in the current review\n",
    "  used = []\n",
    "  for token in tokens:\n",
    "    if token == \"\":\n",
    "      continue\n",
    "    if token not in positive_dictionary:\n",
    "      positive_dictionary[token] = 0\n",
    "    # Incrementing the count for each token only once per review\n",
    "    if token not in used:\n",
    "      positive_dictionary[token] += 1\n",
    "      used.append(token)\n",
    "\n",
    "# Splitting the the negative reviews into training and testing sets (with a random seed)\n",
    "negative_train, negative_test = train_test_split(\n",
    "    negative_reviews, test_size=0.2\n",
    ")\n",
    "\n",
    "for review in negative_train:\n",
    "  tokens = review.split()\n",
    "  # Counting the total number of negative tokens\n",
    "  negative_token_count += len(tokens)\n",
    "  # 'used' is a list to keep track of tokens already seen in the current review\n",
    "  used = []\n",
    "  for token in tokens:\n",
    "    if token == \"\":\n",
    "      continue\n",
    "    if token not in negative_dictionary:\n",
    "      negative_dictionary[token] = 0\n",
    "    # Incrementing the count for each token only once per review\n",
    "    if token not in used:\n",
    "      negative_dictionary[token] += 1\n",
    "      used.append(token)\n",
    "\n",
    "\n",
    "# Testing the classifier with positive test data\n",
    "correct_positive, total_positive = 0, len(positive_test) \n",
    "TP_pos, FN_pos = 0, 0\n",
    "for test_review in positive_test:\n",
    "  # Calculating the prior probability of a review being positive\n",
    "  probability_positive = math.log(positive_count_train / total_count)\n",
    "  # Calculating the prior probability of a review being negative\n",
    "  probability_negative = math.log(negative_count_train / total_count)\n",
    "  tokens = test_review.split()\n",
    "  for token in tokens:\n",
    "    if token == \"\":\n",
    "      continue\n",
    "    # Calculating the likelihood of each token for positive\n",
    "    if token in positive_dictionary:\n",
    "      probability_positive += math.log((positive_dictionary[token] + 1) / (positive_token_count + len(positive_dictionary)))\n",
    "    else:\n",
    "      probability_positive += math.log(1 / (positive_token_count + len(positive_dictionary)))\n",
    "\n",
    "    # Calculating the likelihood of each token for negative\n",
    "    if token in negative_dictionary:\n",
    "      probability_negative += math.log((negative_dictionary[token] + 1) / (negative_token_count + len(negative_dictionary)))\n",
    "    else:\n",
    "      probability_negative += math.log(1 / (negative_token_count + len(negative_dictionary)))\n",
    "     # Check if prediction is correct\n",
    "    # if (probability_positive < probability_negative):\n",
    "    #   TP_pos += 1  # True Positive: correctly identified negative\n",
    "    # else:\n",
    "    #  FN_pos += 1  # False Negative: incorrectly identified negative as positive\n",
    "    true_labels.append('pos')  # The true label for these reviews is 'pos'\n",
    "    predicted_label = 'pos' if probability_positive > probability_negative else 'neg'\n",
    "    predicted_labels.append(predicted_label)\n",
    "  # If the calculated probability for positive is greater, the review is classified as positive  \n",
    "  if (probability_positive > probability_negative):\n",
    "    correct_positive += 1\n",
    "\n",
    "\n",
    "# Testing the classifier with negative test data\n",
    "correct_negative, total_negative = 0, len(negative_test)\n",
    "for test_review in negative_test:\n",
    "  # Calculating the prior probability of a review being positive\n",
    "  probability_positive = math.log(positive_count_train / total_count)\n",
    "  # Calculaing the prior probability of a review being negative\n",
    "  probability_negative = math.log(negative_count_train / total_count)\n",
    "\n",
    "  tokens = test_review.split()\n",
    "  for token in tokens:\n",
    "    if token == \"\":\n",
    "      continue\n",
    "    # Calculating the likelihood of each token for positive\n",
    "    if token in positive_dictionary:\n",
    "      probability_positive += math.log((positive_dictionary[token] + 1) / (positive_token_count + len(positive_dictionary)))\n",
    "    else:\n",
    "      probability_positive += math.log(1 / (positive_token_count + len(positive_dictionary)))\n",
    "    # Calculating the likelihood of each token for negative\n",
    "    if token in negative_dictionary:\n",
    "      probability_negative += math.log((negative_dictionary[token] + 1) / (negative_token_count + len(negative_dictionary)))\n",
    "    else:\n",
    "      probability_negative += math.log(1 / (negative_token_count + len(negative_dictionary)))\n",
    "\n",
    "    true_labels.append('neg')  # The true label for these reviews is 'neg'\n",
    "    predicted_label = 'neg' if probability_positive < probability_negative else 'pos'\n",
    "    predicted_labels.append(predicted_label)\n",
    "\n",
    "  # If the calculated probability for negative is greater, the review is classified as negative  \n",
    "  if (probability_positive < probability_negative):\n",
    "    correct_negative += 1\n",
    "\n",
    "# Calculating total number of correct predictions and total number of predictions\n",
    "total_correct = correct_negative + correct_positive\n",
    "total_predictions = total_negative + total_positive\n",
    "\n",
    "# Generating the classification report\n",
    "# Now that we have the true and predicted labels, we can generate the classification report\n",
    "report = classification_report(true_labels, predicted_labels, target_names=['neg', 'pos'])\n",
    "\n",
    "\n",
    "# Calculating and print the accuracy of the classifier\n",
    "print(\"Accuracy: \" + str(total_correct / total_predictions))\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
